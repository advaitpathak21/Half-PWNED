- Spidering is the automated process of systematically browsing the WWW
- follows links from one page to another
- bots with pre-defined algos to discover and index web pages for web recon and data analysis

![](/attachments/Pasted-image-20241215122433.png)

You might find:
1. Links
2. Comments
3. Metadata
4. Sensitive files

<hr>

## robots.txt
- tells bots which websites can and cannot be crawled
- Can find hidden directories, map website structure, detect crawler traps
- robots.txt is a file in the root directory.
	- `User-agent: *`
	- `Directives` 
		- ![](/attachments/Pasted-image-20241216162947.png)
	- Why follow robots.txt:
		- ![](/attachments/Pasted-image-20241216163530.png)

<hr>

## Well-Known URIs:
- The `/.well-known/` path in the root domain of the website contains config files related to services, protocols and security mechanisms
- IANA registry of `.well-known` URIs - https://www.iana.org/assignments/well-known-uris/well-known-uris.xhtml
- ![](/attachments/Pasted-image-20241216165030.png)
- `openid-configuration` endpoint will return a JSON doc

### Tools:
1. Burp Spider
2. OWASP Zap
3. Scrapy
4. Apache Nutch

- `sudo python3 ReconSpider.py http://inlanefreight.com`

<hr>
